{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install json\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install nltk\n",
    "# !pip install bs4\n",
    "# !pip install re\n",
    "# !pip install contractions\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "# Just for visuals\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nlpaug\n",
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported data\n",
    "df0 = pd.read_pickle('class0.pkl')\n",
    "df1 = pd.read_pickle('class1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a list of all the facts\n",
    "str_lis = df1.facts.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated all the facts\n",
    "long_str = ' '.join(str_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted the nouns from the facts\n",
    "blob = TextBlob(long_str)\n",
    "noun_str = blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_noun = ' '.join(noun_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created list of nouns\n",
    "tokenized = nltk.word_tokenize(long_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = ' '.join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# removed all the non-alphabetic characters if any\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "final_str = regex.sub(' ', long_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our data is case-sensitive, changed all the nouns 1st character to capital\n",
    "final_str\n",
    "result = final_str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokenized = nltk.word_tokenize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented the data using contextual word embeddigs\n",
    "# upto 50% of the non-nouns words have been changed\n",
    "# used bert-base-cased model, since our data is case sensitive\n",
    "# stopwords parameter is the list of words which will be skipped without substituting, where we gave list of nouns\n",
    "# selected top 4 similar words from word embeddings\n",
    "final_facts = []\n",
    "for i in range(len(str_lis)):\n",
    "  TOPK=4 #default=100\n",
    "  ACT = 'substitute' #\"substitute\"\n",
    "  aug_bert = naw.ContextualWordEmbsAug(\n",
    "      model_path='bert-base-cased', aug_p=0.5,\n",
    "      stopwords=final_tokenized,\n",
    "      device='cuda',\n",
    "      action=ACT, top_k=TOPK)\n",
    "  print(i)\n",
    "  #print(\"Original:\")\n",
    "  #print(test_sentence)\n",
    "  #print(\"Augmented Text:\")\n",
    "  final_facts.append(str_lis[i])\n",
    "  for j in range(4):\n",
    "    augmented_text = aug_bert.augment(str_lis[i])\n",
    "    final_facts.append(augmented_text)\n",
    "    #print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_few = df1.drop(['facts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_data = df1_few.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flis = []\n",
    "for i in range(len(lis_data)):\n",
    "  for j in range(5):\n",
    "    flis.append(lis_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.DataFrame(flis, columns =['index', 'ID', 'name', 'href', 'first_party', 'second_party', 'winning_party', 'winner_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factdf = pd.DataFrame(final_facts, columns =['Facts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['Facts'] = factdf['Facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_few1 = df0.drop(['facts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_few1['Facts'] = df0['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = df0_few1.append(fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.to_pickle(\"task1_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
